"""
OpenAI Service - ChatGPT & TTS Integration
Coach Ivy: Your personal Korean companion
"""
import logging
from openai import OpenAI
from config import settings
from pathlib import Path
import hashlib
import tempfile
import os
from typing import Literal, Optional
from fastapi import UploadFile

logger = logging.getLogger(__name__)

# Initialize OpenAI client
client = OpenAI(api_key=settings.openai_api_key)

# ===== COACH IVY SYSTEM PROMPTS =====

COACH_IVY_BASE_PROMPT = """B·∫°n l√† "Coach Ivy", m·ªôt gi√°o vi√™n ti·∫øng H√†n c√° nh√¢n cho ng∆∞·ªùi Vi·ªát h·ªçc ti·∫øng H√†n.

**T√≠nh c√°ch:**
- Th√¢n thi·ªán, khuy·∫øn kh√≠ch v√† h·ªó tr·ª£
- Ki√™n nh·∫´n v√† th·∫•u hi·ªÉu
- Nhi·ªát t√¨nh v·ªõi s·ª± ti·∫øn b·ªô
- Chuy√™n nghi·ªáp nh∆∞ng ·∫•m √°p

**Phong c√°ch giao ti·∫øp:**
- S·ª≠ d·ª•ng ti·∫øng Vi·ªát ƒë·ªÉ gi·∫£i th√≠ch v√† h∆∞·ªõng d·∫´n
- ƒê∆∞a ra v√≠ d·ª• b·∫±ng ti·∫øng H√†n (ÌïúÍµ≠Ïñ¥) k√®m Hangul (ÌïúÍ∏Ä)
- Gi·ªØ c√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn (2-4 c√¢u cho h·∫ßu h·∫øt tr∆∞·ªùng h·ª£p)
- Lu√¥n t√≠ch c·ª±c v√† ƒë·ªông vi√™n
- Bao g·ªìm Hangul (ÌïúÍ∏Ä) khi d·∫°y t·ª´/c·ª•m t·ª´ ti·∫øng H√†n

**Ph∆∞∆°ng ph√°p d·∫°y:**
- T·∫≠p trung v√†o c√°ch s·ª≠ d·ª•ng th·ª±c t·∫ø
- ƒê∆∞a ra v√≠ d·ª• t·ª´ cu·ªôc s·ªëng th·ª±c
- Gi·∫£i th√≠ch "t·∫°i sao" ch·ª© kh√¥ng ch·ªâ "c√°i g√¨"
- Khuy·∫øn kh√≠ch luy·ªán t·∫≠p v√† l·∫∑p l·∫°i
- K·ª∑ ni·ªám nh·ªØng th√†nh c√¥ng nh·ªè
- Gi√∫p ƒë·ª° v·ªÅ ph√°t √¢m v√† ng·ªØ ph√°p ti·∫øng H√†n
"""

MODE_PROMPTS = {
    "free_chat": """Ng∆∞·ªùi d√πng ƒëang c√≥ cu·ªôc tr√≤ chuy·ªán th√¢n m·∫≠t ƒë·ªÉ luy·ªán t·∫≠p ti·∫øng H√†n (ÌïúÍµ≠Ïñ¥).
- Tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch t·ª± nhi√™n b·∫±ng ti·∫øng Vi·ªát
- Nh·∫π nh√†ng s·ª≠a c√°c l·ªói l·ªõn
- Gi·ªØ cu·ªôc tr√≤ chuy·ªán di·ªÖn ra t·ª± nhi√™n
- S·ª≠ d·ª•ng c∆° h·ªôi n√†y ƒë·ªÉ d·∫°y khi ph√π h·ª£p
- Bao g·ªìm Hangul (ÌïúÍ∏Ä) cho t·ª´/c·ª•m t·ª´ ti·∫øng H√†n""",

    "explain": """Ng∆∞·ªùi d√πng c·∫ßn gi√∫p ƒë·ª° ƒë·ªÉ hi·ªÉu m·ªôt kh√°i ni·ªám, t·ª´, ho·∫∑c c·ª•m t·ª´ ti·∫øng H√†n.
- Cung c·∫•p gi·∫£i th√≠ch r√µ r√†ng b·∫±ng ti·∫øng Vi·ªát
- ƒê∆∞a ra 2-3 v√≠ d·ª• th·ª±c t·∫ø k√®m Hangul (ÌïúÍ∏Ä)
- Bao g·ªìm b·∫£n d·ªãch ti·∫øng Vi·ªát cho c√°c thu·∫≠t ng·ªØ quan tr·ªçng
- Gi·ªØ ƒë∆°n gi·∫£n v√† c√≥ th·ªÉ th·ª±c hi·ªán ƒë∆∞·ª£c
- Gi·∫£i th√≠ch c√°c m·∫´u ng·ªØ ph√°p ti·∫øng H√†n khi li√™n quan""",

    "speaking_feedback": """Ng∆∞·ªùi d√πng v·ª´a luy·ªán n√≥i ti·∫øng H√†n. Cung c·∫•p ph·∫£n h·ªìi mang t√≠nh x√¢y d·ª±ng b·∫±ng ti·∫øng Vi·ªát.
- B·∫Øt ƒë·∫ßu v·ªõi l·ªùi ƒë·ªông vi√™n
- Ch·ªâ ra nh·ªØng g√¨ h·ªç l√†m t·ªët
- ƒê·ªÅ xu·∫•t M·ªòT c·∫£i thi·ªán ch√≠nh
- Cung c·∫•p phi√™n b·∫£n ƒë√£ s·ª≠a b·∫±ng Hangul (ÌïúÍ∏Ä)
- ƒê∆∞a ra v√≠ d·ª• t∆∞∆°ng t·ª± ƒë·ªÉ luy·ªán t·∫≠p"""
}


def get_system_prompt(mode: str = "free_chat") -> str:
    """Get complete system prompt for Coach Ivy"""
    mode_specific = MODE_PROMPTS.get(mode, MODE_PROMPTS["free_chat"])
    return f"{COACH_IVY_BASE_PROMPT}\n\n{mode_specific}"


# ===== CHATGPT FUNCTIONS =====

async def chat_with_coach(
    message: str,
    mode: str = "free_chat",
    context: Optional[dict] = None
) -> tuple[str, str]:
    """
    Chat with Coach Ivy

    Args:
        message: User's message
        mode: Conversation mode (free_chat, explain, speaking_feedback)
        context: Additional context (lesson_id, level, etc.)

    Returns:
        tuple: (reply_text, emotion_tag)
    """
    try:
        system_prompt = get_system_prompt(mode)

        # Add context to system prompt if provided
        if context:
            context_str = f"\n\nContext: {context}"
            system_prompt += context_str

        # Call ChatGPT
        response = client.chat.completions.create(
            model=settings.openai_model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": message}
            ],
            temperature=0.7,
            max_tokens=300
        )

        reply = response.choices[0].message.content.strip()

        # Determine emotion tag based on response content
        emotion_tag = _analyze_emotion(reply)

        logger.info(f"Coach Ivy replied (mode={mode}, emotion={emotion_tag})")
        return reply, emotion_tag

    except Exception as e:
        logger.error(f"Error in chat_with_coach: {e}")
        raise


def _analyze_emotion(text: str) -> str:
    """
    Analyze text to determine appropriate emotion tag for avatar

    Returns: neutral | praise | corrective | encouraging
    """
    text_lower = text.lower()

    # Praise indicators
    praise_words = ["excellent", "perfect", "great", "wonderful", "amazing", "fantastic", "correct", "well done", "good job", "tuy·ªát v·ªùi", "ho√†n h·∫£o", "ÌõåÎ•≠Ìï¥", "ÏôÑÎ≤ΩÌï¥"]
    if any(word in text_lower for word in praise_words):
        return "praise"

    # Corrective indicators
    corrective_words = ["however", "but", "correction", "should be", "mistake", "error", "incorrect", "s·ª≠a", "sai", "ÏàòÏ†ï", "ÌãÄÎ†∏Ïñ¥"]
    if any(word in text_lower for word in corrective_words):
        return "corrective"

    # Encouraging indicators
    encouraging_words = ["keep", "practice", "try", "don't worry", "no problem", "keep going", "ti·∫øp t·ª•c", "c·ªë l√™n", "Í≥ÑÏÜç", "ÌôîÏù¥ÌåÖ"]
    if any(word in text_lower for word in encouraging_words):
        return "encouraging"

    return "neutral"


# ===== EXERCISE FEEDBACK =====

async def check_exercise_with_feedback(
    question: str,
    user_answers: list[str],
    correct_answers: list[str],
    exercise_type: str = "multiple_choice"
) -> tuple[bool, float, str, str]:
    """
    Check exercise and generate AI feedback

    Returns:
        tuple: (is_correct, score, feedback_text, emotion_tag)
    """
    try:
        # Calculate correctness
        is_correct = user_answers == correct_answers
        score = 100.0 if is_correct else 0.0

        # Generate AI feedback
        prompt = f"""H·ªçc vi√™n ƒë√£ tr·∫£ l·ªùi c√¢u h·ªèi h·ªçc ti·∫øng H√†n n√†y:
C√¢u h·ªèi: {question}
C√¢u tr·∫£ l·ªùi c·ªßa h·ªç: {' '.join(user_answers)}
ƒê√°p √°n ƒë√∫ng: {' '.join(correct_answers)}

Cung c·∫•p ph·∫£n h·ªìi ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát (2-3 c√¢u):
- N·∫øu ƒë√∫ng: khen ng·ª£i v√† gi·∫£i th√≠ch t·∫°i sao ƒë√∫ng
- N·∫øu sai: nh·∫π nh√†ng gi·∫£i th√≠ch l·ªói v√† cung c·∫•p ƒë√°p √°n ƒë√∫ng k√®m l√Ω do
- Bao g·ªìm Hangul (ÌïúÍ∏Ä) khi gi·∫£i th√≠ch t·ª´/c·ª•m t·ª´ ti·∫øng H√†n"""

        response = client.chat.completions.create(
            model=settings.openai_model_name,
            messages=[
                {"role": "system", "content": get_system_prompt("explain")},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=200
        )

        feedback = response.choices[0].message.content.strip()
        emotion_tag = "praise" if is_correct else "corrective"

        logger.info(f"Exercise checked: correct={is_correct}, score={score}")
        return is_correct, score, feedback, emotion_tag

    except Exception as e:
        logger.error(f"Error in check_exercise_with_feedback: {e}")
        raise


# ===== TTS FUNCTIONS =====

# Directory for storing TTS audio files
MEDIA_DIR = Path("media/tts")
MEDIA_DIR.mkdir(parents=True, exist_ok=True)


def _get_audio_hash(text: str, voice: str) -> str:
    """Generate unique hash for text + voice combination"""
    content = f"{text}_{voice}"
    return hashlib.md5(content.encode()).hexdigest()


async def generate_speech(
    text: str,
    voice: Optional[str] = None
) -> str:
    """
    Generate speech from text using OpenAI TTS

    Args:
        text: Text to convert to speech
        voice: Voice to use (default from settings)

    Returns:
        str: Path to audio file
    """
    try:
        if not voice:
            voice = settings.openai_tts_voice

        # Check cache
        audio_hash = _get_audio_hash(text, voice)
        audio_path = MEDIA_DIR / f"{audio_hash}.mp3"

        # Return cached file if exists
        if audio_path.exists():
            logger.info(f"TTS cache hit for: {text[:50]}...")
            return str(audio_path)

        # Generate new audio
        logger.info(f"Generating TTS for: {text[:50]}...")
        response = client.audio.speech.create(
            model=settings.openai_tts_model,
            voice=voice,
            input=text,
            response_format="mp3"
        )

        # Save to file
        response.stream_to_file(audio_path)
        logger.info(f"TTS saved to: {audio_path}")

        return str(audio_path)

    except Exception as e:
        logger.error(f"Error in generate_speech: {e}")
        raise


# ===== WHISPER (SPEECH-TO-TEXT) FUNCTIONS =====

async def transcribe_audio(
    file: UploadFile,
    language: str = "ko"
) -> str:
    """
    Transcribe audio file to text using OpenAI Whisper

    Args:
        file: Audio file (webm, mp3, wav, etc.)
        language: Language code (default: "ko" for Korean)

    Returns:
        str: Transcribed text
    """
    temp_file_path = None
    try:
        # Create temp file with appropriate extension
        file_extension = Path(file.filename).suffix or ".webm"
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as temp_file:
            temp_file_path = temp_file.name

            # Write uploaded file to temp location
            content = await file.read()
            temp_file.write(content)
            temp_file.flush()

        logger.info(f"Transcribing audio file: {file.filename} ({len(content)} bytes)")

        # Call Whisper API
        with open(temp_file_path, "rb") as audio_file:
            response = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language=language,
                response_format="text"
            )

        transcript = response.strip() if isinstance(response, str) else response.text.strip()
        logger.info(f"Transcription complete: {transcript[:100]}...")

        return transcript

    except Exception as e:
        logger.error(f"Error in transcribe_audio: {e}")
        raise

    finally:
        # Cleanup temp file
        if temp_file_path and os.path.exists(temp_file_path):
            try:
                os.unlink(temp_file_path)
                logger.debug(f"Cleaned up temp file: {temp_file_path}")
            except Exception as e:
                logger.warning(f"Failed to delete temp file: {e}")


# ===== BILINGUAL FEEDBACK FUNCTIONS =====

async def generate_bilingual_feedback(
    expected_text: str,
    spoken_text: str,
    word_accuracy: float,
    accuracy_details: dict,
    model_result: Optional[dict] = None
) -> tuple[str, list[str]]:
    """
    T·∫°o ph·∫£n h·ªìi b·∫±ng ti·∫øng Vi·ªát cho b√†i luy·ªán ph√°t √¢m
    K·∫øt h·ª£p k·∫øt qu·∫£ t·ª´ model pronunciation check (n·∫øu c√≥) v·ªõi GPT

    Args:
        expected_text: VƒÉn b·∫£n ƒë√∫ng
        spoken_text: Nh·ªØng g√¨ ng∆∞·ªùi d√πng ƒë√£ n√≥i
        word_accuracy: T·ª∑ l·ªá ch√≠nh x√°c ·ªü m·ª©c t·ª´
        accuracy_details: C√°c ch·ªâ s·ªë ch√≠nh x√°c chi ti·∫øt
        model_result: K·∫øt qu·∫£ t·ª´ pronunciation model (n·∫øu c√≥) v·ªõi c√°c keys:
            - phoneme_accuracy: ƒê·ªô ch√≠nh x√°c ·ªü m·ª©c phoneme (0-100)
            - per: Phoneme Error Rate (0-1)
            - wrong_phonemes: List of (expected, predicted) tuples
            - wrong_words: List of words with errors

    Returns:
        tuple: (feedback_vi, tricky_words)
    """
    try:
        matches = accuracy_details.get('matches', 0)
        substitutions = accuracy_details.get('substitutions', 0)
        deletions = accuracy_details.get('deletions', 0)
        insertions = accuracy_details.get('insertions', 0)

        # Build prompt v·ªõi th√¥ng tin t·ª´ model (n·∫øu c√≥)
        prompt_parts = [f"""H·ªçc vi√™n ƒë√£ luy·ªán ƒë·ªçc to ti·∫øng H√†n (ÌïúÍµ≠Ïñ¥).

VƒÉn b·∫£n mong ƒë·ª£i: "{expected_text}"
H·ªç ƒë√£ n√≥i: "{spoken_text}"

ƒê·ªô ch√≠nh x√°c t·ª´: {word_accuracy:.1f}%
- T·ª´ ƒë√∫ng: {matches}
- T·ª´ sai: {substitutions}
- T·ª´ thi·∫øu: {deletions}
- T·ª´ th·ª´a: {insertions}"""]

        # Th√™m th√¥ng tin t·ª´ model pronunciation check n·∫øu c√≥
        if model_result:
            phoneme_accuracy = model_result.get('phoneme_accuracy', word_accuracy)
            per = model_result.get('per', 0.0)
            wrong_phonemes = model_result.get('wrong_phonemes', [])
            wrong_words = model_result.get('wrong_words', [])
            
            prompt_parts.append(f"""
üìä K·∫æT QU·∫¢ T·ª™ MODEL PH√ÅT √ÇM CHUY√äN D·ª§NG:
- ƒê·ªô ch√≠nh x√°c phoneme (√¢m v·ªã): {phoneme_accuracy:.1f}%
- Phoneme Error Rate (PER): {per:.4f}
- S·ªë phoneme sai: {len(wrong_phonemes)}
- T·ª´ c√≥ l·ªói: {', '.join(wrong_words[:5]) if wrong_words else 'Kh√¥ng c√≥'}

üîç CHI TI·∫æT L·ªñI PHONEME (n·∫øu c√≥):
""")
            
            if wrong_phonemes:
                for i, (exp, pred) in enumerate(wrong_phonemes[:5], 1):
                    prompt_parts.append(f"  {i}. Mong ƒë·ª£i: '{exp}' ‚Üí ƒê√£ n√≥i: '{pred}'")
            else:
                prompt_parts.append("  Kh√¥ng c√≥ l·ªói phoneme l·ªõn")

        prompt_parts.append("""

T·∫°o ph·∫£n h·ªìi CHI TI·∫æT v√† H·ªÆU √çCH b·∫±ng ti·∫øng Vi·ªát trong ƒë·ªãnh d·∫°ng JSON:

1. "feedback_vi": 2-3 c√¢u b·∫±ng ti·∫øng Vi·ªát
   - B·∫Øt ƒë·∫ßu v·ªõi l·ªùi ƒë·ªông vi√™n t√≠ch c·ª±c
   - N·∫øu c√≥ th√¥ng tin t·ª´ model, gi·∫£i th√≠ch c·ª• th·ªÉ v·ªÅ l·ªói phoneme (v√≠ d·ª•: "B·∫°n ƒë√£ ph√°t √¢m '„Ñ±' th√†nh '„Ñ≤'")
   - Ch·ªâ ra t·ª´ n√†o c·∫ßn luy·ªán t·∫≠p th√™m (n·∫øu c√≥ wrong_words)
   - ƒê∆∞a ra 1-2 l·ªùi khuy√™n c·ª• th·ªÉ ƒë·ªÉ c·∫£i thi·ªán (v√≠ d·ª•: "H√£y ch√∫ √Ω ph√°t √¢m r√µ r√†ng c√°c ph·ª• √¢m cu·ªëi")
   - Gi·ªØ t√≠ch c·ª±c v√† khuy·∫øn kh√≠ch
   - Bao g·ªìm Hangul (ÌïúÍ∏Ä) khi ƒë·ªÅ c·∫≠p ƒë·∫øn t·ª´ ti·∫øng H√†n

2. "tricky_words": M·∫£ng 2-3 t·ª´ ti·∫øng H√†n kh√≥ m√† h·ªçc vi√™n g·∫∑p kh√≥ khƒÉn (t·ª´ wrong_words, bao g·ªìm Hangul)

Output ONLY valid JSON in this exact format:
{{
  "feedback_vi": "...",
  "tricky_words": ["word1", "word2"]
}}""")

        prompt = "\n".join(prompt_parts)

        response = client.chat.completions.create(
            model=settings.openai_model_name,
            messages=[
                {"role": "system", "content": get_system_prompt("speaking_feedback")},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=300
        )

        response_text = response.choices[0].message.content.strip()

        # Parse JSON response
        import json
        try:
            # Try to extract JSON from response (handle cases where GPT adds markdown)
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()

            data = json.loads(response_text)
            feedback_vi = data.get("feedback_vi", "T·ªët l·∫Øm! Ti·∫øp t·ª•c luy·ªán t·∫≠p nh√©.")
            tricky_words = data.get("tricky_words", [])

            logger.info(f"Feedback generated - VI: {len(feedback_vi)} chars")
            return feedback_vi, tricky_words

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON from GPT: {e}")
            logger.error(f"Response was: {response_text}")
            # Fallback to simple feedback
            if word_accuracy >= 85:
                feedback_vi = "Ph√°t √¢m ti·∫øng H√†n r·∫•t t·ªët! Ti·∫øp t·ª•c nh∆∞ v·∫≠y nh√©."
            elif word_accuracy >= 70:
                feedback_vi = "Kh√° t·ªët! Luy·ªán th√™m m·ªôt ch√∫t ƒë·ªÉ ph√°t √¢m ti·∫øng H√†n r√µ r√†ng h∆°n."
            else:
                feedback_vi = "Ti·∫øp t·ª•c luy·ªán t·∫≠p ti·∫øng H√†n! H√£y n√≥i ch·∫≠m v√† r√µ r√†ng h∆°n."

            return feedback_vi, []

    except Exception as e:
        logger.error(f"Error in generate_bilingual_feedback: {e}")
        # Return safe defaults
        return "C·ªë g·∫Øng t·ªët! Ti·∫øp t·ª•c luy·ªán t·∫≠p ti·∫øng H√†n nh√©.", []
